{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"Admission_Predict.csv\")\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Admission_Predict_Ver1.1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (500, 9)\n",
      "Shape (400, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape\", df.shape)\n",
    "print(\"Shape\", df_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         500 non-null    int64  \n",
      " 1   GRE Score          500 non-null    int64  \n",
      " 2   TOEFL Score        500 non-null    int64  \n",
      " 3   University Rating  500 non-null    int64  \n",
      " 4   SOP                500 non-null    float64\n",
      " 5   LOR                500 non-null    float64\n",
      " 6   CGPA               500 non-null    float64\n",
      " 7   Research           500 non-null    int64  \n",
      " 8   Chance of Admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Serial No.\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA',\n",
       "       'Research', 'Chance of Admit '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR  CGPA  Research  \\\n",
       "min        290           92                  1  1.0  1.0  6.80         0   \n",
       "max        340          120                  5  5.0  5.0  9.92         1   \n",
       "\n",
       "     Chance of Admit  \n",
       "min             0.34  \n",
       "max             0.97  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_ft = pd.DataFrame({\"GRE Score\": [df[\"GRE Score\"].min(), df[\"GRE Score\"].max()],\n",
    "                        \"TOEFL Score\": [df[\"TOEFL Score\"].min(), df[\"TOEFL Score\"].max()], \n",
    "                        \"University Rating\":[df[\"University Rating\"].min(), df[\"University Rating\"].max()], \n",
    "                        \"SOP\": [df[\"SOP\"].min(), df[\"SOP\"].max()],\n",
    "                        \"LOR\": [df[\"LOR \"].min(), df[\"LOR \"].max()],\n",
    "                        \"CGPA\": [df[\"CGPA\"].min(), df[\"CGPA\"].max()],\n",
    "                        \"Research\": [df[\"Research\"].min(), df[\"Research\"].max()],\n",
    "                        \"Chance of Admit\": [df[\"Chance of Admit \"].min(), df[\"Chance of Admit \"].max()]\n",
    "                        }, index=[\"min\", \"max\"])\n",
    "\n",
    "range_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA',\n",
       "       'Research', 'Chance of Admit '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Chance of Admit \"]\n",
    "X = df.drop(columns=[\"Chance of Admit \"])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation=\"relu\", input_dim=7))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                128       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273 (1.07 KB)\n",
      "Trainable params: 273 (1.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "10/10 [==============================] - 1s 37ms/step - loss: 0.4953 - val_loss: 0.3402\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.2659 - val_loss: 0.1299\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0937 - val_loss: 0.0429\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0404 - val_loss: 0.0528\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0377 - val_loss: 0.0439\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0292 - val_loss: 0.0295\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0233 - val_loss: 0.0239\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0199 - val_loss: 0.0208\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0174 - val_loss: 0.0182\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0154 - val_loss: 0.0156\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0138\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0120\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0094\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0092\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0089\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0081 - val_loss: 0.0087\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0084\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0082\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0075 - val_loss: 0.0080\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0073 - val_loss: 0.0078\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0071 - val_loss: 0.0076\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0069 - val_loss: 0.0075\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0062 - val_loss: 0.0068\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0061 - val_loss: 0.0068\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0060 - val_loss: 0.0067\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0059 - val_loss: 0.0065\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0058 - val_loss: 0.0064\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0057 - val_loss: 0.0063\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0056 - val_loss: 0.0062\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0054 - val_loss: 0.0061\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0054 - val_loss: 0.0060\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0053 - val_loss: 0.0059\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0051 - val_loss: 0.0057\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0050 - val_loss: 0.0057\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0050 - val_loss: 0.0056\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0050 - val_loss: 0.0055\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.0055\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0047 - val_loss: 0.0053\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.0050\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0050\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0046\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.0046\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0045\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0041 - val_loss: 0.0045\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0039\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 432/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0037\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0035\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=500, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a67981b1d0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9296055], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score is:  0.7915986002190485\n",
      "Mean Absolute Error is:  0.04712621372938156\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 Score is: \", r2_score(y_test, y_pred))\n",
    "print(\"Mean Absolute Error is: \", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Admission_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
